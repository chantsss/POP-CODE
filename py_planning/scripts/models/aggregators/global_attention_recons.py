import torch
import torch.nn as nnfrom models.aggregators.aggregator import PredictionAggregatorfrom typing import Dict, Tuplefrom models.aggregators.global_attention import GlobalAttention


class GlobalAttentionRecons(PredictionAggregator):
    """
    Aggregate context encoding using scaled dot product attention. Query obtained using target agent encoding,
    Keys and values obtained using map and surrounding agent encodings.
    """

    def __init__(self, args: Dict):

        """
        args to include

        enc_size: int Dimension of encodings generated by encoder
        emb_size: int Size of embeddings used for queries, keys and values
        num_heads: int Number of attention heads

        """
        super().__init__()
        self.init_globalattenion = GlobalAttention(args)
        self.recons_globalattenion = GlobalAttention(args)
        self.cross_attention_by_layer = args['cross_attention_by_layer']
        self.cross_attention_by_concat = args['cross_attention_by_concat']
        if self.cross_attention_by_layer:
            self.cross_attention_layer = torch.nn.MultiheadAttention(args)

    def forward(self, encodings: Dict) -> torch.Tensor:
        """
        Forward pass for attention aggregator
        """
        op = {}
        op['init_op'] = self.init_globalattenion(encodings['init_encodings']) 
        op['recons_op'] = self.init_globalattenion(encodings['init_encodings']) 
        if self.cross_attention_by_layer:
            op['refine_op'] = self.cross_attention_layer(op['init_op'], op['recons_op'])
        op['teacher_output'] = encodings['teacher_output']

        return op
